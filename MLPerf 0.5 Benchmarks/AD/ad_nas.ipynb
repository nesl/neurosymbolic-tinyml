{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b833de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import glob\n",
    "import sys\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto() \n",
    "config.gpu_options.allow_growth = True  \n",
    "config.log_device_placement = True  \n",
    "sess2 = tf.compat.v1.Session(config=config)\n",
    "set_session(sess2) \n",
    "from math import ceil\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, AveragePooling1D, Conv1D, Activation, UpSampling1D, Reshape\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from tcn import compiled_tcn\n",
    "from mango.tuner import Tuner\n",
    "from keras_flops import get_flops\n",
    "from tqdm import tqdm\n",
    "import common as com\n",
    "import keras_model\n",
    "import eval_functions_eembc\n",
    "from sklearn import metrics\n",
    "from hardware_utils import *\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0264bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAS_EPOCHS = 50 #NAS epochs\n",
    "device = \"NUCLEO_L4R5ZI_P\" #hardware name\n",
    "dirpath='AD_Mbed_Prog/' #mbed program directory\n",
    "model_name = \"ADmodel\"+device+\"_\"+\".h5\"\n",
    "platform_connected = True #HIL or proxy\n",
    "quantization = True #use quantization\n",
    "os.system(\"mkdir -p trained_models/\")\n",
    "\n",
    "log_file_name = 'log_NAS_AD'+str(platform_connected)+\"_\"+device+'.csv' #log file for NAS\n",
    "if os.path.exists(log_file_name):\n",
    "    os.remove(log_file_name)\n",
    "row_write = ['score', 'accuracy','SRAM','Flash','Latency',\n",
    "             'nb_filters','kernel_size','dilations','nb_stacks','use_skip_connections']\n",
    "with open(log_file_name, 'a', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_write)\n",
    "if os.path.exists(log_file_name[0:-4]+'.p'):\n",
    "    os.remove(log_file_name[0:-4]+'.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39caa8af",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2eaf29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 13:49:43,735 - INFO - load_directory <- development\n",
      "2022-09-27 13:49:43,736 - INFO - target_dir : /home/nesl/swapnil/thin-bayes/Anomaly Detection Benchmark/l4r5zi_p_h/dev_data/ToyCar\n",
      "2022-09-27 13:49:43,744 - INFO - train_file num : 4000\n",
      "generate train_dataset: 100%|██████████████| 4000/4000 [00:34<00:00, 114.41it/s]\n"
     ]
    }
   ],
   "source": [
    "param = com.yaml_load()\n",
    "os.makedirs(param[\"model_directory\"], exist_ok=True)\n",
    "dirs = com.select_dirs(param=param, mode=1)\n",
    "idx = 0\n",
    "target_dir = dirs[0]\n",
    "files = com.file_list_generator(target_dir)\n",
    "train_data = com.list_to_vector_array(files,\n",
    "                                  msg=\"generate train_dataset\",\n",
    "                                  n_mels=param[\"feature\"][\"n_mels\"],\n",
    "                                  frames=param[\"feature\"][\"frames\"],\n",
    "                                  n_fft=param[\"feature\"][\"n_fft\"],\n",
    "                                  hop_length=param[\"feature\"][\"hop_length\"],\n",
    "                                  power=param[\"feature\"][\"power\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfba646",
   "metadata": {},
   "source": [
    "## Training and NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a5be5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_NN(nb_filters=6,kernel_size=32,dilations=[1,2,4,8,16,32,64,128],\n",
    "                 nb_stacks=1, use_skip_connections = True, platform_connected = False, quantization=True):\n",
    "    print(nb_filters,kernel_size,dilations,nb_stacks, use_skip_connections)\n",
    "    training_flag = 0\n",
    "    score = -5.0\n",
    "    accuracy = -1.0\n",
    "    ################################################################################\n",
    "    i = Input((param[\"feature\"][\"n_mels\"] * param[\"feature\"][\"frames\"]))\n",
    "    x = Reshape((1,param[\"feature\"][\"n_mels\"] * param[\"feature\"][\"frames\"]))(i)\n",
    "    tcn_enc = TCN(nb_filters=nb_filters, kernel_size=kernel_size, nb_stacks=nb_stacks, dilations=dilations, \n",
    "                  padding='same', use_skip_connections=use_skip_connections, return_sequences=True,\n",
    "                  kernel_initializer='glorot_normal', name='tcn-enc')(x)\n",
    "    enc_flat = Conv1D(filters=ceil(nb_filters/4), kernel_size=ceil(kernel_size/3), activation='linear', padding='same')(tcn_enc)\n",
    "    dec_upsample = Conv1D(filters=ceil(nb_filters/4), kernel_size=ceil(kernel_size/3), activation='linear', padding='same')(enc_flat)\n",
    "    dec_reconstructed = TCN(nb_filters=nb_filters, kernel_size=kernel_size, nb_stacks=nb_stacks, dilations=dilations, \n",
    "                  padding='same', use_skip_connections=use_skip_connections, return_sequences=False,\n",
    "                  kernel_initializer='glorot_normal', name='tcn-dec')(dec_upsample)\n",
    "    o = Dense(param[\"feature\"][\"n_mels\"] * param[\"feature\"][\"frames\"], activation='linear')(dec_reconstructed)\n",
    "    model = Model(inputs=[i], outputs=[o])\n",
    "    ################################################################################\n",
    "    model.compile(**param[\"fit\"][\"compile\"])\n",
    "    maxRAM, maxFlash = return_hardware_specs(device)\n",
    "    if(platform_connected == False):\n",
    "        Latency = get_flops(model, batch_size=1) #latency proxy\n",
    "        RAM = get_model_memory_usage(batch_size=1, model=model) #SRAM proxy\n",
    "        if(quantization==True):\n",
    "            RAM = RAM/8.0\n",
    "        Flash =  get_model_flash_usage(model,'g_model',quantization=quantization,train_data=train_data) #flash proxy\n",
    "        if(RAM < maxRAM and Flash < maxFlash):\n",
    "            training_flag = 1\n",
    "        else:\n",
    "            training_flag = 0\n",
    "            score = -5.0\n",
    "            accuracy = -1.0\n",
    "    else:\n",
    "        RAM, Flash, Latency, err_flag = platform_in_the_loop_controller(model,'g_model',\n",
    "                                            device,train_data,dir_path=dirpath,quantization=quantization)\n",
    "        if(RAM!=-1 and Flash!=-1 and Latency!=-1):\n",
    "            training_flag = 1\n",
    "        else:\n",
    "            training_flag = 0\n",
    "            score = -5.0\n",
    "            accuracy = -1.0\n",
    "            \n",
    "    if(training_flag == 1):\n",
    "        history = model.fit(train_data,\n",
    "                            train_data,\n",
    "                            epochs=param[\"fit\"][\"epochs\"],\n",
    "                            batch_size=param[\"fit\"][\"batch_size\"],\n",
    "                            shuffle=param[\"fit\"][\"shuffle\"],\n",
    "                            validation_split=param[\"fit\"][\"validation_split\"],\n",
    "                            verbose=param[\"fit\"][\"verbose\"])\n",
    "        accuracy = 1/history.history['val_loss'][-1]\n",
    "        if(platform_connected == False):\n",
    "            score = accuracy + 0.01*((RAM/maxRAM) + (Flash/maxFlash)) +  0.01*(Latency/1e6)\n",
    "        else:\n",
    "            score = accuracy + 0.01*((RAM/maxRAM) + (Flash/maxFlash)) +  0.01*(Latency/0.8)\n",
    "    \n",
    "    row_write = [score, accuracy,RAM,Flash,Latency,\n",
    "             nb_filters,kernel_size,dilations,nb_stacks,use_skip_connections]\n",
    "    print('Design choice:',row_write)\n",
    "    with open(log_file_name, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(row_write)     \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2fb605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_res(data, file_name):\n",
    "    pickle.dump( data, open( file_name, \"wb\" ) )\n",
    "    \n",
    "min_layer = 3\n",
    "max_layer = 8\n",
    "a_list = [1,2,4,8,16,32,64,128]\n",
    "all_combinations = []\n",
    "dil_list = []\n",
    "for r in range(len(a_list) + 1):\n",
    "    combinations_object = itertools.combinations(a_list, r)\n",
    "    combinations_list = list(combinations_object)\n",
    "    all_combinations += combinations_list\n",
    "all_combinations = all_combinations[1:]\n",
    "for item in all_combinations:\n",
    "    if(len(item) >= min_layer and len(item) <= max_layer):\n",
    "        dil_list.append(list(item))\n",
    "\n",
    "param_dict = {\n",
    "    'nb_filters': range(3,64),\n",
    "    'nb_stacks': [1,2,3],\n",
    "    'kernel_size': range(3,16),\n",
    "    'use_skip_connections': [True, False],\n",
    "    'dilations': dil_list\n",
    "}\n",
    "\n",
    "def objfunc(args_list):\n",
    "\n",
    "    objective_evaluated = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for hyper_par in args_list:\n",
    "        nb_filters = hyper_par['nb_filters']\n",
    "        nb_stacks = hyper_par['nb_stacks']\n",
    "        kernel_size = hyper_par['kernel_size']\n",
    "        use_skip_connections = hyper_par['use_skip_connections']\n",
    "        dilations = hyper_par['dilations']\n",
    "            \n",
    "        objective = objective_NN(nb_filters=nb_filters,kernel_size=kernel_size,dilations=dilations,\n",
    "                 nb_stacks=nb_stacks, use_skip_connections = True, \n",
    "                                 platform_connected = platform_connected, quantization=quantization)\n",
    "        objective_evaluated.append(objective)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print('objective:', objective, ' time:',end_time-start_time)\n",
    "        \n",
    "    return objective_evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93878917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 7 [1, 4, 16, 64] 2 True\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Lambda object at 0x7fe996ca8850>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpaiy3mmlp/assets\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Lambda object at 0x7fe996ca8850>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpr6t1grlm/assets\n"
     ]
    }
   ],
   "source": [
    "conf_Dict = dict()\n",
    "conf_Dict['batch_size'] = 1 \n",
    "conf_Dict['num_iteration'] = NAS_EPOCHS\n",
    "conf_Dict['initial_random']= 5\n",
    "tuner = Tuner(param_dict, objfunc,conf_Dict)\n",
    "all_runs = []\n",
    "results = tuner.maximize()\n",
    "all_runs.append(results)\n",
    "save_res(all_runs,log_file_name[0:-4]+'.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5a3ed0",
   "metadata": {},
   "source": [
    "## Train the Best Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb7b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filters = results['best_params']['nb_filters']\n",
    "nb_stacks = results['best_params']['nb_stacks']\n",
    "kernel_size = results['best_params']['kernel_size']\n",
    "use_skip_connections = results['best_params']['use_skip_connections']\n",
    "dilations = results['best_params']['dilations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7151cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input((param[\"feature\"][\"n_mels\"] * param[\"feature\"][\"frames\"]))\n",
    "x = Reshape((1,param[\"feature\"][\"n_mels\"] * param[\"feature\"][\"frames\"]))(i)\n",
    "tcn_enc = TCN(nb_filters=nb_filters, kernel_size=kernel_size, nb_stacks=nb_stacks, dilations=dilations, \n",
    "              padding='same', use_skip_connections=use_skip_connections, return_sequences=True,\n",
    "              kernel_initializer='glorot_normal', name='tcn-enc')(x)\n",
    "enc_flat = Conv1D(filters=ceil(nb_filters/4), kernel_size=ceil(kernel_size/3), activation='linear', padding='same')(tcn_enc)\n",
    "dec_upsample = Conv1D(filters=ceil(nb_filters/4), kernel_size=ceil(kernel_size/3), activation='linear', padding='same')(enc_flat)\n",
    "dec_reconstructed = TCN(nb_filters=nb_filters, kernel_size=kernel_size, nb_stacks=nb_stacks, dilations=dilations, \n",
    "              padding='same', use_skip_connections=use_skip_connections, return_sequences=True,\n",
    "              kernel_initializer='glorot_normal', name='tcn-dec')(dec_upsample)\n",
    "o = Dense(param[\"feature\"][\"n_mels\"] * param[\"feature\"][\"frames\"], activation='linear')(dec_reconstructed)\n",
    "model = Model(inputs=[i], outputs=[o])\n",
    "################################################################################\n",
    "model.compile(**param[\"fit\"][\"compile\"])\n",
    "history = model.fit(train_data,\n",
    "                    train_data,\n",
    "                    epochs=param[\"fit\"][\"epochs\"],\n",
    "                    batch_size=param[\"fit\"][\"batch_size\"],\n",
    "                    shuffle=param[\"fit\"][\"shuffle\"],\n",
    "                    validation_split=param[\"fit\"][\"validation_split\"],\n",
    "                    verbose=param[\"fit\"][\"verbose\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_models/\" + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696270f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_id_list = com.get_machine_id_list_for_test(target_dir)\n",
    "csv_lines = []\n",
    "csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
    "performance = []\n",
    "for id_str in machine_id_list:\n",
    "    test_files, y_true = com.test_file_list_generator(target_dir, id_str, \"development\")\n",
    "    anomaly_score_list = []\n",
    "    y_pred = [0. for k in test_files]\n",
    "    for file_idx, file_path in tqdm(enumerate(test_files), total=len(test_files)):\n",
    "        data = com.file_to_vector_array(file_path,\n",
    "                                        n_mels=param[\"feature\"][\"n_mels\"],\n",
    "                                        frames=param[\"feature\"][\"frames\"],\n",
    "                                        n_fft=param[\"feature\"][\"n_fft\"],\n",
    "                                        hop_length=param[\"feature\"][\"hop_length\"],\n",
    "                                        power=param[\"feature\"][\"power\"])\n",
    "        pred = model.predict(data)\n",
    "        errors = numpy.mean(numpy.square(data - pred), axis=1)\n",
    "        y_pred[file_idx] = numpy.mean(errors)\n",
    "        anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n",
    "        \n",
    "    auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "    p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=param[\"max_fpr\"])\n",
    "    csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
    "    performance.append([auc, p_auc])\n",
    "    acc_eembc = eval_functions_eembc.calculate_ae_accuracy(y_pred, y_true)\n",
    "    pr_acc_eembc = eval_functions_eembc.calculate_ae_pr_accuracy(y_pred, y_true)\n",
    "    auc_eembc = eval_functions_eembc.calculate_ae_auc(y_pred, y_true, \"dummy\")\n",
    "\n",
    "averaged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\n",
    "csv_lines.append([\"Average\"] + list(averaged_performance))\n",
    "csv_lines.append([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
