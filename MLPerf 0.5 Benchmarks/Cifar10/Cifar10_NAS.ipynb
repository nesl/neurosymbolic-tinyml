{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto() \n",
    "config.gpu_options.allow_growth = True  \n",
    "config.log_device_placement = True  \n",
    "sess2 = tf.compat.v1.Session(config=config)\n",
    "set_session(sess2) \n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "from mango.tuner import Tuner\n",
    "from scipy.stats import uniform\n",
    "from keras_flops import get_flops\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import eval_functions_eembc\n",
    "from train_utils import *\n",
    "from hardware_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec78c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500 #model epochs\n",
    "NAS_EPOCHS = 50 #NAS epochs\n",
    "BS = 32 #batch size\n",
    "cifar_10_dir = 'cifar-10-batches-py/' #cifar-10-dataset-directory\n",
    "device = \"NUCLEO_L4R5ZI_P\" #hardware name\n",
    "dirpath='Cifar10_Mbed_Prog/' #mbed program directory\n",
    "model_name = \"trainedResnet_\"+device+\"_\"+\".h5\"\n",
    "platform_connected = False #HIL or proxy\n",
    "quantization = True #use quantization\n",
    "os.system(\"mkdir -p trained_models/\")\n",
    "\n",
    "\n",
    "log_file_name = 'log_NAS_Cifar10_'+str(platform_connected)+\"_\"+device+'.csv' #log file for NAS\n",
    "if os.path.exists(log_file_name):\n",
    "    os.remove(log_file_name)\n",
    "row_write = ['score', 'accuracy','SRAM','Flash','Latency',\n",
    "             'nstacks','num_filters','kernel_size','batch_norm','act']\n",
    "with open(log_file_name, 'a', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_write)\n",
    "if os.path.exists(log_file_name[0:-4]+'.p'):\n",
    "    os.remove(log_file_name[0:-4]+'.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e7467",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf26a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"python3 perf_samples_loader.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004b99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_filenames, train_labels, test_data, test_filenames, test_labels, label_names = \\\n",
    "    load_cifar_10_data(cifar_10_dir)\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    #brightness_range=(0.9, 1.2),\n",
    "    #contrast_range=(0.9, 1.2),\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Train filenames: \", train_filenames.shape)\n",
    "print(\"Train labels: \", train_labels.shape)\n",
    "print(\"Test data: \", test_data.shape)\n",
    "print(\"Test filenames: \", test_filenames.shape)\n",
    "print(\"Test labels: \", test_labels.shape)\n",
    "print(\"Label names: \", label_names.shape)\n",
    "\n",
    "num_plot = 5\n",
    "f, ax = plt.subplots(num_plot, num_plot)\n",
    "for m in range(num_plot):\n",
    "    for n in range(num_plot):\n",
    "        idx = np.random.randint(0, train_data.shape[0])\n",
    "        ax[m, n].imshow(train_data[idx])\n",
    "        ax[m, n].get_xaxis().set_visible(False)\n",
    "        ax[m, n].get_yaxis().set_visible(False)\n",
    "f.subplots_adjust(hspace=0.1)\n",
    "f.subplots_adjust(wspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fbd0b5",
   "metadata": {},
   "source": [
    "## Training and NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af65381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_NN(nstacks=3,init_num_filter=16,kernel_size=3,\n",
    "                 use_batch_norm=True, use_act = True, platform_connected = False, quantization=True):\n",
    "    print(nstacks,init_num_filter,kernel_size,use_batch_norm,use_act)\n",
    "    training_flag = 0\n",
    "    score = -5.0\n",
    "    accuracy = -1.0\n",
    "    model = generate_resnet(nstacks=nstacks,init_num_filter=init_num_filter,kernel_size=kernel_size,\n",
    "                 use_batch_norm=use_batch_norm, use_act = use_act,num_classes=10)\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics='accuracy', loss_weights=None,\n",
    "    weighted_metrics=None, run_eagerly=None )\n",
    "    maxRAM, maxFlash = return_hardware_specs(device)\n",
    "    if(platform_connected == False):\n",
    "        Latency = get_flops(model, batch_size=1) #latency proxy\n",
    "        RAM = get_model_memory_usage(batch_size=1, model=model) #SRAM proxy\n",
    "        if(quantization==True):\n",
    "            RAM = RAM/8.0\n",
    "        Flash =  get_model_flash_usage(model,'pretrainedResnet',quantization=quantization) #flash proxy\n",
    "        if(RAM < maxRAM and Flash < maxFlash):\n",
    "            training_flag = 1\n",
    "        else:\n",
    "            training_flag = 0\n",
    "            score = -5.0\n",
    "            accuracy = -1.0\n",
    "    else:\n",
    "        RAM, Flash, Latency, err_flag = platform_in_the_loop_controller(model,'pretrainedResnet', \n",
    "                                            device,dir_path=dirpath,quantization=quantization)\n",
    "        if(RAM!=-1 and Flash!=-1 and Latency!=-1):\n",
    "            training_flag = 1\n",
    "        else:\n",
    "            training_flag = 0\n",
    "            score = -5.0\n",
    "            accuracy = -1.0\n",
    "            \n",
    "    if(training_flag == 1):\n",
    "        history = model.fit(datagen.flow(train_data, train_labels, batch_size=BS),\n",
    "          steps_per_epoch=len(train_data) / BS, epochs=EPOCHS, callbacks=[lr_scheduler])\n",
    "        accuracy = history.history['accuracy'][-1]\n",
    "        if(platform_connected == False):\n",
    "            score = accuracy + 0.01*((RAM/maxRAM) + (Flash/maxFlash)) +  0.01*(Latency/1e6)\n",
    "        else:\n",
    "            score = accuracy + 0.01*((RAM/maxRAM) + (Flash/maxFlash)) +  0.01*(Latency/0.8)\n",
    "    \n",
    "    row_write = [score, accuracy,RAM,Flash,Latency,\n",
    "             nstacks,init_num_filter,kernel_size,use_batch_norm,use_act]\n",
    "    print('Design choice:',row_write)\n",
    "    with open(log_file_name, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(row_write)     \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_res(data, file_name):\n",
    "    pickle.dump( data, open( file_name, \"wb\" ) )\n",
    "    \n",
    "param_dict = {\n",
    "    'nstacks': range(1,5),\n",
    "    'init_num_filter': [2,4,6,8,10,12,14,16,18,20,22,24],\n",
    "    'kernel_size': [1,3,5,7],\n",
    "    'use_batch_norm': [True, False],\n",
    "    'use_act': [True, False]\n",
    "}\n",
    "\n",
    "def objfunc(args_list):\n",
    "\n",
    "    objective_evaluated = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for hyper_par in args_list:\n",
    "        nstacks = hyper_par['nstacks']\n",
    "        init_num_filter = hyper_par['init_num_filter']\n",
    "        kernel_size = hyper_par['kernel_size']\n",
    "        use_batch_norm = hyper_par['use_batch_norm']\n",
    "        use_act = hyper_par['use_act']\n",
    "            \n",
    "        objective = objective_NN(nstacks=nstacks,init_num_filter=init_num_filter,kernel_size=kernel_size,\n",
    "                 use_batch_norm=use_batch_norm, use_act = use_act, \n",
    "                platform_connected = platform_connected, quantization=quantization)\n",
    "        objective_evaluated.append(objective)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print('objective:', objective, ' time:',end_time-start_time)\n",
    "        \n",
    "    return objective_evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2caa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_Dict = dict()\n",
    "conf_Dict['batch_size'] = 1 \n",
    "conf_Dict['num_iteration'] = NAS_EPOCHS\n",
    "conf_Dict['initial_random']= 5\n",
    "tuner = Tuner(param_dict, objfunc,conf_Dict)\n",
    "all_runs = []\n",
    "results = tuner.maximize()\n",
    "all_runs.append(results)\n",
    "save_res(all_runs,log_file_name[0:-4]+'.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c1329f",
   "metadata": {},
   "source": [
    "## Train Best Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb47f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nstacks = results['best_params']['nstacks']\n",
    "init_num_filter = results['best_params']['init_num_filter']\n",
    "kernel_size = results['best_params']['kernel_size']\n",
    "use_batch_norm = results['best_params']['use_batch_norm']\n",
    "use_act = results['best_params']['use_act']\n",
    "\n",
    "model = generate_resnet(nstacks=nstacks,init_num_filter=init_num_filter,kernel_size=kernel_size,\n",
    "                 use_batch_norm=use_batch_norm, use_act = use_act,num_classes=10)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics='accuracy', loss_weights=None,\n",
    "    weighted_metrics=None, run_eagerly=None )\n",
    "history = model.fit(datagen.flow(train_data, train_labels, batch_size=BS),\n",
    "          steps_per_epoch=len(train_data) / BS, epochs=EPOCHS, callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_models/\" + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_filenames, train_labels, test_data, test_filenames, test_labels, label_names = \\\n",
    "    load_cifar_10_data(cifar_10_dir)\n",
    "_idxs = np.load('perf_samples_idxs.npy')\n",
    "test_data = test_data[_idxs]\n",
    "test_labels = test_labels[_idxs]\n",
    "test_filenames = test_filenames[_idxs]\n",
    "\n",
    "print(\"Test data: \", test_data.shape)\n",
    "print(\"Test filenames: \", test_filenames.shape)\n",
    "print(\"Test labels: \", test_labels.shape)\n",
    "print(\"Label names: \", label_names.shape)\n",
    "label_classes = np.argmax(test_labels,axis=1)\n",
    "print(\"Label classes: \", label_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('trained_models/' + model_name)\n",
    "\n",
    "test_metrics = model.evaluate(x=test_data, y=test_labels, batch_size=32, verbose=1, return_dict=True)\n",
    "\n",
    "print(\"Performances on cifar10 test set\")\n",
    "print(\"Keras evaluate method\")\n",
    "print(\"Accuracy keras: \", test_metrics['accuracy'])\n",
    "print(\"---------------------\")\n",
    "\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "print(\"EEMBC calculate_accuracy method\")\n",
    "accuracy_eembc = eval_functions_eembc.calculate_accuracy(predictions, label_classes)\n",
    "print(\"---------------------\")\n",
    "\n",
    "auc_scikit = roc_auc_score(test_labels, predictions)\n",
    "print(\"sklearn.metrics.roc_auc_score method\")\n",
    "print(\"AUC sklearn: \", auc_scikit)\n",
    "print(\"---------------------\")\n",
    "\n",
    "print(\"EEMBC calculate_auc method\")\n",
    "auc_eembc = eval_functions_eembc.calculate_auc(predictions, label_classes, label_names, model_name)\n",
    "print(\"---------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
