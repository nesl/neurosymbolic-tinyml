{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fcf398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gtda.time_series import SlidingWindow\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto() \n",
    "config.gpu_options.allow_growth = True  \n",
    "config.log_device_placement = True  \n",
    "sess2 = tf.compat.v1.Session(config=config)\n",
    "set_session(sess2) \n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_hub as hub\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from tcn import compiled_tcn\n",
    "from mango.tuner import Tuner\n",
    "from keras_flops import get_flops\n",
    "from hardware_utils import *\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import itertools\n",
    "import random\n",
    "import get_dataset as kws_data\n",
    "import kws_util\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6888cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 60 #model epochs\n",
    "NAS_EPOCHS = 50 #NAS epochs\n",
    "kws_data_dir = 'kws_data/' #cifar-10-dataset-directory\n",
    "device = \"NUCLEO_L4R5ZI_P\" #hardware name\n",
    "dirpath='KWS_Mbed_Prog/' #mbed program directory\n",
    "model_name = \"kwsmodel_\"+device+\"_\"+\".h5\"\n",
    "platform_connected = True #HIL or proxy\n",
    "quantization = True #use quantization\n",
    "os.system(\"mkdir -p kws_data/\")\n",
    "os.system(\"mkdir -p trained_models/\")\n",
    "\n",
    "log_file_name = 'log_NAS_kws'+str(platform_connected)+\"_\"+device+'.csv' #log file for NAS\n",
    "if os.path.exists(log_file_name):\n",
    "    os.remove(log_file_name)\n",
    "row_write = ['score', 'accuracy','SRAM','Flash','Latency',\n",
    "             'nb_filters','kernel_size','dilations','nb_stacks','use_skip_connections']\n",
    "with open(log_file_name, 'a', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_write)\n",
    "if os.path.exists(log_file_name[0:-4]+'.p'):\n",
    "    os.remove(log_file_name[0:-4]+'.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38156111",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d36e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flags, unparsed = kws_util.parse_command()\n",
    "Flags.data_dir = kws_data_dir\n",
    "print('We will download data to {:}'.format(Flags.data_dir))\n",
    "ds_train, ds_test, ds_val = kws_data.get_training_data(Flags)\n",
    "print(\"Done getting data\")\n",
    "train_shuffle_buffer_size = 85511\n",
    "val_shuffle_buffer_size = 10102\n",
    "test_shuffle_buffer_size = 4890\n",
    "\n",
    "ds_train = ds_train.shuffle(train_shuffle_buffer_size)\n",
    "ds_val = ds_val.shuffle(val_shuffle_buffer_size)\n",
    "ds_test = ds_test.shuffle(test_shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045982e2",
   "metadata": {},
   "source": [
    "## Training and NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size_samples = int(Flags.sample_rate* Flags.window_size_ms / 1000)\n",
    "window_stride_samples = int(Flags.sample_rate * Flags.window_stride_ms / 1000)\n",
    "desired_samples = int(Flags.sample_rate * Flags.clip_duration_ms / 1000)\n",
    "length_minus_window = (desired_samples - window_size_samples)\n",
    "spectrogram_length = 1 + int(length_minus_window / window_stride_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae824e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_NN(nb_filters=6,kernel_size=32,dilations=[1,2,4,8,16,32,64,128],\n",
    "                 nb_stacks=1, use_skip_connections = True, platform_connected = False, quantization=True):\n",
    "    print(nb_filters,kernel_size,dilations,nb_stacks, use_skip_connections)\n",
    "    training_flag = 0\n",
    "    score = -5.0\n",
    "    accuracy = -1.0\n",
    "    ################################################################################\n",
    "    inputs = layers.Input((spectrogram_length,Flags.dct_coefficient_count,1))\n",
    "    x = inputs\n",
    "    x = layers.Reshape((spectrogram_length,Flags.dct_coefficient_count))(x)\n",
    "    x = TCN(return_sequences=False,\n",
    "                     nb_filters=nb_filters,\n",
    "                     kernel_size=kernel_size,\n",
    "                     dilations=dilations,\n",
    "                     nb_stacks=nb_stacks,\n",
    "                     use_weight_norm=False,\n",
    "                     use_skip_connections=use_skip_connections)(x)\n",
    "    outputs = layers.Dense(12, activation=\"softmax\", name=\"pred\")(x)\n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    ################################################################################\n",
    "    callbacks = kws_util.get_callbacks(args=Flags)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "    losses = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    metrics = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    model.compile(optimizer=optimizer, loss=losses, metrics=metrics)\n",
    "    maxRAM, maxFlash = return_hardware_specs(device)\n",
    "    if(platform_connected == False):\n",
    "        Latency = get_flops(model, batch_size=1) #latency proxy\n",
    "        RAM = get_model_memory_usage(batch_size=1, model=model) #SRAM proxy\n",
    "        if(quantization==True):\n",
    "            RAM = RAM/8.0\n",
    "        Flash =  get_model_flash_usage(model,'g_kws_model_data',quantization=quantization) #flash proxy\n",
    "        if(RAM < maxRAM and Flash < maxFlash):\n",
    "            training_flag = 1\n",
    "        else:\n",
    "            training_flag = 0\n",
    "            score = -5.0\n",
    "            accuracy = -1.0\n",
    "            \n",
    "    if(training_flag == 1):\n",
    "        history = model.fit(ds_train, validation_data=ds_val, epochs=EPOCHS, callbacks=callbacks)\n",
    "        accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "        if(platform_connected == False):\n",
    "            score = accuracy + 0.01*((RAM/maxRAM) + (Flash/maxFlash)) +  0.01*(Latency/1e6)\n",
    "        else:\n",
    "            score = accuracy + 0.01*((RAM/maxRAM) + (Flash/maxFlash)) +  0.01*(Latency/0.8)\n",
    "    \n",
    "    row_write = [score, accuracy,RAM,Flash,Latency,\n",
    "             nb_filters,kernel_size,dilations,nb_stacks,use_skip_connections]\n",
    "    print('Design choice:',row_write)\n",
    "    with open(log_file_name, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(row_write)     \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_res(data, file_name):\n",
    "    pickle.dump( data, open( file_name, \"wb\" ) )\n",
    "    \n",
    "min_layer = 3\n",
    "max_layer = 8\n",
    "a_list = [1,2,4,8,16,32,64,128,256]\n",
    "all_combinations = []\n",
    "dil_list = []\n",
    "for r in range(len(a_list) + 1):\n",
    "    combinations_object = itertools.combinations(a_list, r)\n",
    "    combinations_list = list(combinations_object)\n",
    "    all_combinations += combinations_list\n",
    "all_combinations = all_combinations[1:]\n",
    "for item in all_combinations:\n",
    "    if(len(item) >= min_layer and len(item) <= max_layer):\n",
    "        dil_list.append(list(item))\n",
    "\n",
    "param_dict = {\n",
    "    'nb_filters': range(2,64),\n",
    "    'nb_stacks': [1,2,3],\n",
    "    'kernel_size': range(2,16),\n",
    "    'use_skip_connections': [True, False],\n",
    "    'dilations': dil_list\n",
    "}\n",
    "\n",
    "def objfunc(args_list):\n",
    "\n",
    "    objective_evaluated = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for hyper_par in args_list:\n",
    "        nb_filters = hyper_par['nb_filters']\n",
    "        nb_stacks = hyper_par['nb_stacks']\n",
    "        kernel_size = hyper_par['kernel_size']\n",
    "        use_skip_connections = hyper_par['use_skip_connections']\n",
    "        dilations = hyper_par['dilations']\n",
    "            \n",
    "        objective = objective_NN(nb_filters=nb_filters,kernel_size=kernel_size,dilations=dilations,\n",
    "                 nb_stacks=nb_stacks, use_skip_connections = True, \n",
    "                                 platform_connected = platform_connected, quantization=quantization)\n",
    "        objective_evaluated.append(objective)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print('objective:', objective, ' time:',end_time-start_time)\n",
    "        \n",
    "    return objective_evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc867e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_Dict = dict()\n",
    "conf_Dict['batch_size'] = 1 \n",
    "conf_Dict['num_iteration'] = NAS_EPOCHS\n",
    "conf_Dict['initial_random']= 5\n",
    "tuner = Tuner(param_dict, objfunc,conf_Dict)\n",
    "all_runs = []\n",
    "results = tuner.maximize()\n",
    "all_runs.append(results)\n",
    "save_res(all_runs,log_file_name[0:-4]+'.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f8f8f",
   "metadata": {},
   "source": [
    "## Train and evaluate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filters = results['best_params']['nb_filters']\n",
    "nb_stacks = results['best_params']['nb_stacks']\n",
    "kernel_size = results['best_params']['kernel_size']\n",
    "use_skip_connections = results['best_params']['use_skip_connections']\n",
    "dilations = results['best_params']['dilations']\n",
    "################################################################################\n",
    "inputs = layers.Input((spectrogram_length,Flags.dct_coefficient_count,1))\n",
    "x = inputs\n",
    "x = layers.Reshape((spectrogram_length,Flags.dct_coefficient_count))(x)\n",
    "x = TCN(return_sequences=False,\n",
    "                 nb_filters=nb_filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 dilations=dilations,\n",
    "                 nb_stacks=nb_stacks,\n",
    "                 use_weight_norm=False,\n",
    "                 use_skip_connections=use_skip_connections)(x)\n",
    "outputs = layers.Dense(12, activation=\"softmax\", name=\"pred\")(x)\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "################################################################################\n",
    "callbacks = kws_util.get_callbacks(args=Flags)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "losses = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "model.compile(optimizer=optimizer, loss=losses, metrics=metrics)\n",
    "history = model.fit(ds_train, validation_data=ds_val, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c41b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_models/\" + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = model.evaluate(ds_test)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
