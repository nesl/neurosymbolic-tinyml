{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gtda.time_series import SlidingWindow\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import random\n",
    "import itertools\n",
    "import sys\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import helpermethods\n",
    "from edgeml_tf.trainer.bonsaiTrainer import BonsaiTrainer\n",
    "from edgeml_tf.graph.bonsai import Bonsai\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from scipy.stats import uniform\n",
    "from data_utils import *\n",
    "import re\n",
    "from mango.tuner import Tuner\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "from tensorflow.keras.losses import MSE\n",
    "from edgeml_tf.tflite.bonsaiLayer import BonsaiLayer\n",
    "#from edgeml_tf.tflite.bonsaiLayerMod import BonsaiLayerMod \n",
    "from tensorflow import keras\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import subprocess\n",
    "import re\n",
    "np.random.seed(42)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 100\n",
    "window_size = 550\n",
    "stride = 50\n",
    "epsilon = 0.4\n",
    "model_dir = 'trainedModels2/'\n",
    "f = '/home/nesl/earable/Earable/Activity_Dataset/' #dataset directory\n",
    "\n",
    "command = [\"python\", \"-c\", \n",
    "           \"from adv_accu import *; get_adv_accu(model_dir = 'trainedModels2/',data_dir = '/home/nesl/earable/Earable/Activity_Dataset/', epsilon=0.4,window_size=550, stride=50,val=False)\"]\n",
    "\n",
    "\n",
    "X_tr, Y_tr, X_test, Y_test = import_auritus_activity_dataset(dataset_folder = f, \n",
    "                                use_timestamp=False, \n",
    "                                shuffle=True, \n",
    "                                window_size = window_size, stride = stride, \n",
    "                                return_test_set = True, test_set_size = 300,channels=2)\n",
    "\n",
    "\n",
    "random_indices = np.random.choice(X_tr.shape[0], size=1000, replace=False)\n",
    "X_val = X_tr[random_indices,:,:]\n",
    "Y_val = Y_tr[random_indices,:]\n",
    "X_tr = np.delete(X_tr,random_indices,axis=0)\n",
    "Y_tr = np.delete(Y_tr,random_indices,axis=0)\n",
    "\n",
    "print(X_tr.shape)\n",
    "print(Y_tr.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_size = 10\n",
    "featX_tr = np.zeros((X_tr.shape[0],feat_size))\n",
    "featX_test = np.zeros((X_test.shape[0],feat_size))\n",
    "featX_val = np.zeros((X_val.shape[0],feat_size))\n",
    "for i in range(X_tr.shape[0]):\n",
    "    cur_win = X_tr[i]\n",
    "    featX_tr[i,0] = np.min(cur_win[:,0])\n",
    "    featX_tr[i,1] = np.min(cur_win[:,1])\n",
    "    featX_tr[i,2] = np.max(cur_win[:,0])\n",
    "    featX_tr[i,3] = np.max(cur_win[:,1])\n",
    "    featX_tr[i,4] = featX_tr[i,2]-featX_tr[i,0]\n",
    "    featX_tr[i,5] = featX_tr[i,3]-featX_tr[i,1]\n",
    "    featX_tr[i,6] = np.var(cur_win[:,0])\n",
    "    featX_tr[i,7] = np.var(cur_win[:,1])\n",
    "    featX_tr[i,8] = np.sqrt(featX_tr[i,6])\n",
    "    featX_tr[i,9] = np.sqrt(featX_tr[i,7])  \n",
    "    \n",
    "for i in range(X_test.shape[0]):\n",
    "    cur_win = X_test[i]\n",
    "    featX_test[i,0] = np.min(cur_win[:,0])\n",
    "    featX_test[i,1] = np.min(cur_win[:,1])\n",
    "    featX_test[i,2] = np.max(cur_win[:,0])\n",
    "    featX_test[i,3] = np.max(cur_win[:,1])\n",
    "    featX_test[i,4] = featX_test[i,2]-featX_test[i,0]\n",
    "    featX_test[i,5] = featX_test[i,3]-featX_test[i,1]\n",
    "    featX_test[i,6] = np.var(cur_win[:,0])\n",
    "    featX_test[i,7] = np.var(cur_win[:,1])\n",
    "    featX_test[i,8] = np.sqrt(featX_test[i,6])\n",
    "    featX_test[i,9] = np.sqrt(featX_test[i,7])\n",
    "    \n",
    "for i in range(X_val.shape[0]):\n",
    "    cur_win = X_val[i]\n",
    "    featX_val[i,0] = np.min(cur_win[:,0])\n",
    "    featX_val[i,1] = np.min(cur_win[:,1])\n",
    "    featX_val[i,2] = np.max(cur_win[:,0])\n",
    "    featX_val[i,3] = np.max(cur_win[:,1])\n",
    "    featX_val[i,4] = featX_val[i,2]-featX_val[i,0]\n",
    "    featX_val[i,5] = featX_val[i,3]-featX_val[i,1]\n",
    "    featX_val[i,6] = np.var(cur_win[:,0])\n",
    "    featX_val[i,7] = np.var(cur_win[:,1])\n",
    "    featX_val[i,8] = np.sqrt(featX_val[i,6])\n",
    "    featX_val[i,9] = np.sqrt(featX_val[i,7])\n",
    "    \n",
    "dataDimension = featX_tr.shape[1]\n",
    "numClasses = Y_tr.shape[1]\n",
    "Xtrain = featX_tr\n",
    "Ytrain = Y_tr\n",
    "Xtest = featX_test\n",
    "Ytest = Y_test\n",
    "Xval = featX_val\n",
    "Yval = Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalEpochs = 1000 #epochs to train each model for\n",
    "bayesEpochs = 50 #epochs for hyperparameter tuning\n",
    "log_file_name = 'Hyperparams_Bonsai_noadv.csv'\n",
    "if os.path.exists(log_file_name):\n",
    "    os.remove(log_file_name)\n",
    "if os.path.exists(log_file_name[0:-4]+'.p'):\n",
    "    os.remove(log_file_name[0:-4]+'.p')\n",
    "row_write = ['score', 'accuracy','adv_accu','Flash','Epoch','Sigma','Depth','ProjectionDimension']\n",
    "with open(log_file_name, 'a', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_NN(sigma = 1.0, depth = 3, projectionDimension = 22,numClasses=9):\n",
    "    \n",
    "    #Fixed hyperparameters:\n",
    "    \n",
    "    #Regularizers for Bonsai Parameters\n",
    "    regZ = 0.0001\n",
    "    regW = 0.001\n",
    "    regV = 0.001\n",
    "    regT = 0.001\n",
    "\n",
    "    learningRate = 0.01\n",
    "    outFile = None\n",
    "    #Sparsity for Bonsai Parameters. x => 100*x % are non-zeros\n",
    "    sparZ = 0.2\n",
    "    sparW = 0.3\n",
    "    sparV = 0.3\n",
    "    sparT = 0.62\n",
    "    batchSize = np.maximum(100, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n",
    "    useMCHLoss = True #(False = Cross Entropy)\n",
    "    #Bonsai uses one classier for Binary, thus this condition\n",
    "    if numClasses == 2:\n",
    "        numClasses = 1\n",
    "        \n",
    "    X = tf.placeholder(\"float32\", [None, dataDimension])\n",
    "    Y = tf.placeholder(\"float32\", [None, numClasses])\n",
    "    \n",
    "    dataDir = model_dir\n",
    "    shutil.rmtree(dataDir, ignore_errors=True)\n",
    "    os.mkdir(dataDir)\n",
    "    currDir = helpermethods.createTimeStampDir(dataDir)\n",
    "    helpermethods.dumpCommand(sys.argv, currDir)\n",
    "    \n",
    "    #Instantiating the Bonsai Graph which will be used for training and inference.\n",
    "    bonsaiObj = Bonsai(numClasses, dataDimension, projectionDimension, depth, sigma)\n",
    "    \n",
    "    #Instantiating the Bonsai Trainer which will be used for 3 phase training.\n",
    "    bonsaiTrainer = BonsaiTrainer(bonsaiObj, regW, regT, regV, regZ, sparW, sparT, sparV, sparZ,\n",
    "                              learningRate, X, Y, useMCHLoss, outFile)\n",
    "    #Session declaration and variable initialization. Interactive Session doesn't clog the entire GPU.\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #The method to to run the 3 phase training, followed by giving out the best early stopping model, accuracy along with saving of the parameters.\n",
    "    bonsaiTrainer.train(batchSize, totalEpochs, sess,\n",
    "                    Xtrain, Xval, Ytrain, Yval, dataDir, currDir)\n",
    "    \n",
    "    model_size = bonsaiTrainer.getModelSize()[1] #flash usage\n",
    "    res_file =  open(model_dir+\"TFBonsaiResults.txt\",\"r\").read()\n",
    "    accu = float(re.findall(\"\\d+\\.\\d+\", res_file[0:20])[0]) #accuracy\n",
    "    ep = int(re.findall(\"\\d+\", res_file[[m.start() for m in re.finditer('totalEpochs', res_file)][0]:[m.start() \n",
    "                                for m in re.finditer('ModelSize', res_file)][0]])[0]) #epoch at which max test accuracy happened\n",
    "    \n",
    "    adv_accu = 'inf'\n",
    "    \n",
    "    score = 10*accu - 0.001*model_size #+ 5*adv_accu  #you can weigh the score to take into account model size too\n",
    "    row_write = [score, accu,adv_accu,model_size,ep,sigma,depth,projectionDimension]\n",
    "    with open(log_file_name, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(row_write)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def save_res(data, file_name):\n",
    "    pickle.dump(data, open(file_name, \"wb\" ))\n",
    "    \n",
    "param_dict = {\n",
    "    'sigma': uniform(1,4),\n",
    "    'depth': [1,2,3,4,5,6],\n",
    "    'projectionDimension': np.arange(10,70)\n",
    "}\n",
    "\n",
    "def objfunc(args_list):\n",
    "\n",
    "    objective_evaluated = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for hyper_par in args_list:\n",
    "        sigma = hyper_par['sigma']\n",
    "        depth = hyper_par['depth']\n",
    "        projectionDimension = hyper_par['projectionDimension']\n",
    "            \n",
    "        objective = objective_NN(sigma=sigma,depth=depth,\n",
    "                                 projectionDimension=projectionDimension,numClasses=numClasses)\n",
    "        objective_evaluated.append(objective)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print('objective:', objective, ' time:',end_time-start_time)\n",
    "        \n",
    "    return objective_evaluated\n",
    "\n",
    "conf_Dict = dict()\n",
    "conf_Dict['batch_size'] = 1 \n",
    "conf_Dict['num_iteration'] = bayesEpochs\n",
    "conf_Dict['initial_random']= 5\n",
    "tuner = Tuner(param_dict, objfunc,conf_Dict)\n",
    "all_runs = []\n",
    "results = tuner.maximize()\n",
    "all_runs.append(results)\n",
    "save_res(all_runs,log_file_name[0:-4]+'.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the best model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = results['best_params']['depth']\n",
    "projectionDimension = results['best_params']['projectionDimension']\n",
    "sigma = results['best_params']['sigma']\n",
    "\n",
    "regZ = 0.0001\n",
    "regW = 0.001\n",
    "regV = 0.001\n",
    "regT = 0.001\n",
    "\n",
    "learningRate = 0.01\n",
    "outFile = None\n",
    "#Sparsity for Bonsai Parameters. x => 100*x % are non-zeros\n",
    "sparZ = 0.2\n",
    "sparW = 0.3\n",
    "sparV = 0.3\n",
    "sparT = 0.62\n",
    "batchSize = np.maximum(100, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n",
    "useMCHLoss = True #(False = Cross Entropy)\n",
    "#Bonsai uses one classier for Binary, thus this condition\n",
    "if numClasses == 2:\n",
    "    numClasses = 1\n",
    "\n",
    "X = tf.placeholder(\"float32\", [None, dataDimension])\n",
    "Y = tf.placeholder(\"float32\", [None, numClasses])\n",
    "\n",
    "dataDir = model_dir\n",
    "shutil.rmtree(dataDir, ignore_errors=True)\n",
    "os.mkdir(dataDir)\n",
    "currDir = helpermethods.createTimeStampDir(dataDir)\n",
    "helpermethods.dumpCommand(sys.argv, currDir)\n",
    "\n",
    "#Instantiating the Bonsai Graph which will be used for training and inference.\n",
    "bonsaiObj = Bonsai(numClasses, dataDimension, projectionDimension, depth, sigma)\n",
    "\n",
    "#Instantiating the Bonsai Trainer which will be used for 3 phase training.\n",
    "bonsaiTrainer = BonsaiTrainer(bonsaiObj, regW, regT, regV, regZ, sparW, sparT, sparV, sparZ,\n",
    "                          learningRate, X, Y, useMCHLoss, outFile)\n",
    "#Session declaration and variable initialization. Interactive Session doesn't clog the entire GPU.\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#The method to to run the 3 phase training, followed by giving out the best early stopping model, accuracy along with saving of the parameters.\n",
    "bonsaiTrainer.train(batchSize, totalEpochs, sess,\n",
    "                Xtrain, Xtest, Ytrain, Ytest, dataDir, currDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,error  = subprocess.Popen(\n",
    "            command, universal_newlines=True,\n",
    "            stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n",
    "\n",
    "adv_accu = float(re.findall(\"\\d+\\.\\d+\",output[output.find('Adversarial Accuracy:'):-1])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = bonsaiTrainer.getModelSize()[1] #flash usage\n",
    "res_file =  open(model_dir+\"TFBonsaiResults.txt\",\"r\").read()\n",
    "accu = float(re.findall(\"\\d+\\.\\d+\", res_file[0:20])[0]) #accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
